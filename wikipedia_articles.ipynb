{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data set: [wikidata](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs \n",
    "import tensorflow as tf \n",
    "\n",
    "# helper libs \n",
    "import numpy as np # matrix maths \n",
    "import random # for randomness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of text\n",
      "Text :  \n",
      " = Valkyria Chronicles III = \n",
      " \n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . Char\n"
     ]
    }
   ],
   "source": [
    "# load dataset \n",
    "\n",
    "text = open('./wikitext-2/wiki.train.tokens', 'rb').read().decode('utf-8')\n",
    "print(\"Lenght of text\".format(len(text)))\n",
    "\n",
    "# print first 1000 chars of test and train \n",
    "print(\"Text :\", text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of chars 283\n",
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '~', '¡', '£', '¥', '§', '°', '±', '²', '³', 'µ', '·', '½', 'Á', 'Å', 'Æ', 'É', 'Í', 'Î', 'Ö', '×', 'Ø', 'Ú', 'Ü', 'Þ', 'à', 'á', 'â', 'ã', 'ä', 'å', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ñ', 'ò', 'ó', 'ô', 'ö', 'ø', 'ú', 'û', 'ü', 'Ā', 'ā', 'ă', 'ć', 'č', 'Đ', 'đ', 'ė', 'ī', 'Ł', 'ł', 'ń', 'Ō', 'ō', 'ś', 'ş', 'š', 'ū', 'ų', 'Ż', 'ž', 'ơ', 'ư', 'ʻ', 'ʿ', '̃', 'α', 'β', 'γ', 'κ', 'μ', 'С', 'а', 'в', 'е', 'к', 'о', 'с', 'т', 'я', 'ا', 'ح', 'ص', 'ل', 'ن', 'ه', '्', 'ก', 'ง', 'ณ', 'ต', 'ม', 'ย', 'ร', 'ล', 'ั', 'า', 'ิ', '่', '์', 'გ', 'დ', 'ვ', 'ზ', 'ი', 'კ', 'ო', 'რ', 'ს', 'უ', 'ც', 'ძ', 'წ', 'ხ', 'ჯ', '჻', 'ḥ', 'ṃ', 'ṅ', 'ṣ', 'ṭ', 'ṯ', 'ả', 'ấ', 'ầ', 'ắ', 'ễ', 'ệ', 'ị', 'ớ', 'ử', 'ỳ', '‑', '–', '—', '‘', '’', '“', '”', '„', '†', '…', '′', '″', '⁄', '₤', '€', '₹', '⅓', '⅔', '→', '−', '≤', '☉', '♭', '♯', '〈', '〉', 'の', 'ァ', 'ア', 'キ', 'ス', 'ッ', 'ト', 'プ', 'ュ', 'リ', 'ル', 'ヴ', '・', '動', '場', '大', '戦', '攻', '機', '殻', '火', '礮', '空', '隊', '\\ufeff', '～']\n"
     ]
    }
   ],
   "source": [
    "# list all the unique chars for the list \n",
    "chars = sorted(list(set(text)))\n",
    "chars_size = len(chars)\n",
    "\n",
    "# print lenght \n",
    "print(\"Lenght of chars\", chars_size)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chars to id mappind \n",
    "chars2id = dict((c, i) for i, c in enumerate(chars))\n",
    "id2chars = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def a helper function to generate probablities of next chars \n",
    "def sample(prediction):\n",
    "    r = random.uniform(0,1)\n",
    "    s = 0\n",
    "    char_id = len(prediction)-1\n",
    "    \n",
    "    for i in range(len(prediction)):\n",
    "        s += prediction[i]\n",
    "        \n",
    "        if s >= r:\n",
    "            char_id = i\n",
    "            break\n",
    "    char_one_hot = np.zeros(shape=[chars_size])\n",
    "    char_one_hot[char_id] = 1.0\n",
    "    \n",
    "    return char_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : [[[0. 1. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "y: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# vectorize our data \n",
    "len_per_section = 50\n",
    "skip = 50\n",
    "sections = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, 10000*100, skip):\n",
    "    sections.append(text[i: i + len_per_section])\n",
    "    next_chars.append(text[i + len_per_section])\n",
    "    \n",
    "\n",
    "# vectorize our chars \n",
    "X = np.zeros((len(sections), len_per_section, chars_size))\n",
    "y = np.zeros((len(sections), chars_size))\n",
    "\n",
    "for i, section in enumerate(sections):\n",
    "    for j, char in enumerate(section):\n",
    "        X[i, j, chars2id[char]] = 1\n",
    "    y[i, chars2id[next_chars[i]]] = 1\n",
    "print(\"X :\", X)\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size  20000\n",
      "Approx steps per training epoch 39\n"
     ]
    }
   ],
   "source": [
    "# Deep Learning Part \n",
    "batch_size = 512\n",
    "num_epochs = 1000\n",
    "log_step = 100\n",
    "save_model = 200\n",
    "hidden_nodes = 1024\n",
    "# start string for the test \n",
    "test_start = 'I am thinking'\n",
    "\n",
    "checkpoint_dir = \"ckpt\"\n",
    "# check if our model checkpoint exits\n",
    "if tf.gfile.Exists(checkpoint_dir):\n",
    "    tf.gfile.DeleteRecursively(checkpoint_dir)\n",
    "tf.gfile.MakeDirs(checkpoint_dir)\n",
    "\n",
    "print(\"Training data size \", len(X))\n",
    "print(\"Approx steps per training epoch\", len(X) // batch_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start building our model \n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    global_step = tf.Variable(0)\n",
    "\n",
    "    # placeholder \n",
    "    data = tf.placeholder(tf.float32, [batch_size, len_per_section, chars_size])\n",
    "    labels = tf.placeholder(tf.float32, [batch_size, chars_size])\n",
    "\n",
    "    # defining all the gates \n",
    "    with tf.name_scope(\"input_gate\"):\n",
    "        w_ii = tf.Variable(tf.truncated_normal([chars_size, hidden_nodes], stddev=0.1), name=\"Input_Weights\")\n",
    "        w_io = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], stddev=0.1), name=\"Output_Weights\")\n",
    "        b_i = tf.Variable(tf.truncated_normal([1, hidden_nodes], stddev=0.1), name=\"Input_Biases\")\n",
    "\n",
    "    with tf.name_scope(\"forget_gate\"):\n",
    "        w_fi = tf.Variable(tf.truncated_normal([chars_size, hidden_nodes], stddev=0.1), name=\"Input_Weights\")\n",
    "        w_fo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], stddev=0.1), name=\"Output_Weights\")\n",
    "        b_f = tf.Variable(tf.truncated_normal([1, hidden_nodes], stddev=0.1), name=\"Forget_Biases\")\n",
    "\n",
    "    with tf.name_scope(\"output_gate\"):\n",
    "        w_oi = tf.Variable(tf.truncated_normal([chars_size, hidden_nodes], stddev=0.1), name=\"Input_Weights\")\n",
    "        w_oo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], stddev=0.1), name=\"Output_Weights\")\n",
    "        b_o = tf.Variable(tf.truncated_normal([1, hidden_nodes], stddev=0.1), name=\"Output_Biases\")\n",
    "\n",
    "    with tf.name_scope(\"memory_gate\"):\n",
    "        w_ci = tf.Variable(tf.truncated_normal([chars_size, hidden_nodes], stddev=0.1), name=\"Input_Weights\")\n",
    "        w_co = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], stddev=0.1), name=\"Output_Weights\")\n",
    "        b_c = tf.Variable(tf.truncated_normal([1, hidden_nodes], stddev=0.1), name=\"Memory_Biases\")\n",
    "        \n",
    "    # helper function for the lstm cell \n",
    "    def lstm(i, o, state):\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, w_ii) + tf.matmul(o, w_io) + b_i)\n",
    "\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, w_fi) + tf.matmul(o, w_fo) + b_f)\n",
    "\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, w_oi) + tf.matmul(o, w_oo) + b_o)\n",
    "\n",
    "        memory_cell = tf.sigmoid(tf.matmul(i, w_ci) + tf.matmul(o, w_co) + b_c)\n",
    "\n",
    "\n",
    "        state = forget_gate * state + memory_cell + input_gate\n",
    "\n",
    "        output = output_gate * tf.tanh(state)\n",
    "\n",
    "        return output, state\n",
    "    \n",
    "    output = tf.zeros([batch_size, hidden_nodes])\n",
    "    state = tf.zeros([batch_size, hidden_nodes])\n",
    "    \n",
    "#     # unroll lstm layer \n",
    "#     for i in range(len_per_section):\n",
    "#         output, state = lstm(data[:, i, :], output, state)\n",
    "        \n",
    "#         if i == 0:\n",
    "#             output_all_i = output\n",
    "#             labels_all_i = data[:, i+1, :]\n",
    "#         output = tf.zeros([batch_size, hidden_nodes])\n",
    "#     state = tf.zeros([batch_size, hidden_nodes])\n",
    "\n",
    "    #unrolled LSTM loop\n",
    "    #for each input set\n",
    "    for i in range(len_per_section):\n",
    "        #calculate state and output from LSTM\n",
    "        output, state = lstm(data[:, i, :], output, state)\n",
    "        #to start, \n",
    "        if i == 0:\n",
    "            #store initial output and labels\n",
    "            outputs_all_i = output\n",
    "            labels_all_i = data[:, i+1, :]\n",
    "        #for each new set, concat outputs and labels\n",
    "        elif i != len_per_section - 1:\n",
    "            #concatenates (combines) vectors along a dimension axis, not multiply\n",
    "            outputs_all_i = tf.concat(axis=0, values=[outputs_all_i, output])\n",
    "            labels_all_i = tf.concat(axis=0, values=[labels_all_i, data[:, i+1, :]])\n",
    "        else:\n",
    "            #final store\n",
    "            outputs_all_i = tf.concat(axis=0, values=[outputs_all_i, output])\n",
    "            labels_all_i = tf.concat(axis=0, values=[labels_all_i, labels])\n",
    "            \n",
    "    #Classifier\n",
    "    w = tf.Variable(tf.truncated_normal([hidden_nodes, chars_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([chars_size]))\n",
    "    \n",
    "    logits = tf.matmul(outputs_all_i, w) + b\n",
    "    \n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_all_i))\n",
    "\n",
    "    #Optimizer\n",
    "    #minimize loss with graident descent, learning rate 10,  keep track of batches\n",
    "    optimizer = tf.train.GradientDescentOptimizer(10.).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 0: 6.64\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    #standard init step\n",
    "    tf.global_variables_initializer().run()\n",
    "    offset = 0\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #for each training step\n",
    "    for step in range(num_epochs):\n",
    "        \n",
    "        #starts off as 0\n",
    "        offset = offset % len(X)\n",
    "        \n",
    "        #calculate batch data and labels to feed model iteratively\n",
    "        if offset <= (len(X) - batch_size):\n",
    "            #first part\n",
    "            batch_data = X[offset: offset + batch_size]\n",
    "            batch_labels = y[offset: offset + batch_size]\n",
    "            offset += batch_size\n",
    "        #until when offset  = batch size, then we \n",
    "        else:\n",
    "            #last part\n",
    "            to_add = batch_size - (len(X) - offset)\n",
    "            batch_data = np.concatenate((X[offset: len(X)], X[0: to_add]))\n",
    "            batch_labels = np.concatenate((y[offset: len(X)], y[0: to_add]))\n",
    "            offset = to_add\n",
    "        \n",
    "        #optimize!!\n",
    "        _, training_loss = sess.run([optimizer, loss], feed_dict={data: batch_data, labels: batch_labels})\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print('training loss at step %d: %.2f' % (step, training_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
